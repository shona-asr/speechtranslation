# -*- coding: utf-8 -*-
"""app_api (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_sDZMpFFEqhyF9_GoWgm2tqi1cPk6ccX
"""

# Install required packages
!pip install -q flask flask-cors faster-whisper torch pyngrok google-cloud-translate google-cloud-texttospeech librosa soundfile

# Import libraries
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import torch
from faster_whisper import WhisperModel
import pyngrok
import logging
import numpy as np
from io import BytesIO
from collections import deque
import os
from google.cloud import translate_v2 as translate
from google.cloud import texttospeech
from google.colab import drive
import librosa
import base64
from tempfile import NamedTemporaryFile
import io
import wave

# Initialize Flask app
app = Flask(__name__)

# ========================
# SECURE SETUP
# ========================

# Mount Google Drive
drive.mount('/content/drive')

# Set Google credentials
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/content/drive/MyDrive/Colab Notebooks/asrGCAPI.json"

# ========================
# APP CONFIGURATION
# ========================

# Configure CORS
CORS(app,
     resources={r"/*": {  # You can use r"/*" too
         "origins": [
             "http://127.0.0.1:3000",
             "http://localhost:3000"
         ]
     }},
     supports_credentials=True
)

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Supported languages
LANGUAGES = {
    'auto': 'auto',
    'autodetect': 'auto',
    'shona': 'sn',
    'sn': 'sn',
    'english': 'en',
    'en': 'en',
    'chinese': 'zh',
    'zh': 'zh',
    'zh-cn': 'zh-CN',
    'zh-tw': 'zh-TW',
    'mandarin': 'zh',
    'ndebele': 'nr',
    'nr': 'nr'
}

# Voice mapping for TTS
TTS_VOICE_MAP = {
    'english': 'en-US',
    'chinese': 'cmn-CN',
    'shona': 'af-ZA',
    'ndebele': 'af-ZA'
}

# ========================
# SERVICE INITIALIZATION
# ========================

# Load Whisper model
model = WhisperModel(
    "realtime-speech/shona-finetune-ct2",
    device="cuda" if torch.cuda.is_available() else "cpu",
    compute_type="float16"
)

# Context buffer for streaming
transcription_context = deque(maxlen=5)

# Initialize Google clients
translate_client = translate.Client()
tts_client = texttospeech.TextToSpeechClient()

# ========================
# HELPER FUNCTIONS
# ========================

def preprocess_audio(audio_file):
    """Convert audio to 16kHz mono float32 numpy array"""
    try:
        # Check if audio_file is a BytesIO object
        if isinstance(audio_file, io.BytesIO):
            # If it's a BytesIO object, save it to a temporary file
            with NamedTemporaryFile(suffix='.wav', delete=False) as temp:
                temp.write(audio_file.read())  # Write the content of BytesIO to temp file
                temp.close()  # Close the temp file so librosa can read it

            # Load with librosa
            audio_np, sr = librosa.load(temp.name, sr=16000, mono=True)
            os.unlink(temp.name)  # Clean up the temporary file

        else:
            # If it's already a file-like object (e.g., from request.files), save it directly
            with NamedTemporaryFile(suffix='.wav', delete=False) as temp:
                audio_file.save(temp.name)

            # Load with librosa
            audio_np, sr = librosa.load(temp.name, sr=16000, mono=True)
            os.unlink(temp.name)  # Clean up the temporary file

        return audio_np.astype(np.float32)

    except Exception as e:
        logger.error(f"Audio processing failed: {str(e)}")
        raise

# Endpoint to get user stats (simplified for now)
@app.route('/user-stats', methods=['GET', 'OPTIONS'])
def user_stats():
    if request.method == 'OPTIONS':
        return '', 200
    return jsonify({"status": "ok"})

# Text to Speech endpoint
@app.route('/text-to-speech', methods=['POST', 'OPTIONS'])
def text_to_speech():
    if request.method == 'OPTIONS':
        return '', 200
    try:
        data = request.get_json()
        text = data.get('text')
        language = data.get('language')

        if not text or not language:
            return jsonify({"error": "Missing text or language"}), 400

        lang_code = TTS_VOICE_MAP.get(language, 'en-US')
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code=lang_code,
            ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
        )
        audio_config = texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.MP3
        )
        response = tts_client.synthesize_speech(
            input=synthesis_input,
            voice=voice,
            audio_config=audio_config
        )
        audio_bytes = response.audio_content

        # Use send_file to stream the audio file in the response
        return send_file(
            io.BytesIO(audio_bytes),
            mimetype='audio/mp3',
            as_attachment=True,
            download_name="output.mp3"
        )

    except Exception as e:
        logger.error(f"TTS error: {str(e)}")
        return jsonify({"error": str(e)}), 500

transcription_context = []

@app.route('/transcribe', methods=['POST', 'OPTIONS'])
def transcribe():
    if request.method == 'OPTIONS':
        return '', 200
    try:
        transcription_context.clear()

        if 'audio' not in request.files:
            return jsonify({"error": "No audio file provided"}), 400

        audio_file = request.files['audio']
        language = request.form.get('language', 'auto')

        # Read and encode audio
        audio_bytes = audio_file.read()
        audio_file.seek(0)
        audio_b64 = base64.b64encode(audio_bytes).decode()

        # Preprocess the audio
        audio_data = preprocess_audio(BytesIO(audio_bytes))

        # Transcribe using faster-whisper with word timestamps
        segments_gen, info = model.transcribe(
            audio_data,
            language=None if language.lower() in ['auto', 'autodetect'] else language,
            beam_size=5,
            best_of=5,
            temperature=0.0,
            word_timestamps=True,
            initial_prompt=""
        )

        # Convert generator to list
        segments = list(segments_gen)

        # Join transcription
        transcription = " ".join(segment.text for segment in segments if hasattr(segment, "text")).strip()
        transcription_context.extend(segments)

        return jsonify({
            "originalAudio": audio_b64,
            "language": info.language,
            "transcription": transcription,
            "confidence": float(np.exp(info.avg_logprob)) if hasattr(info, 'avg_logprob') else 0.0,
            "words": [
                {
                    "segment": segment.text,
                    "words": [
                        {"word": word.word, "start": word.start, "end": word.end}
                        for word in segment.words
                    ] if segment.words else []
                } for segment in segments
            ]
        })

    except Exception as e:
        logger.error(f"Transcription error: {str(e)}")
        return jsonify({"error": str(e)}), 500


# Streaming Transcription endpoint (handles real-time streaming of audio)
@app.route('/transcribe_stream', methods=['POST', 'OPTIONS'])
def transcribe_stream():
    if request.method == 'OPTIONS':
        return '', 200
    try:
        transcription_context.clear()  # Automatically reset context

        if 'audio_chunk' not in request.files:
            return jsonify({"error": "No audio chunk provided"}), 400

        audio_chunk = request.files['audio_chunk']
        language = request.form.get('language', 'auto')

        # Read and encode audio
        audio_bytes = audio_chunk.read()
        audio_chunk.seek(0)
        audio_b64 = base64.b64encode(audio_bytes).decode()

        # Process audio data
        audio_data = preprocess_audio(BytesIO(audio_bytes))

        segments, info = model.transcribe(
            audio_data,
            language=None if language.lower() in ['auto', 'autodetect'] else language,
            beam_size=3,
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=500),
            word_timestamps=True,
            initial_prompt=""
        )

        results = []
        full_transcription = ""
        for segment in segments:
            transcription_context.append(segment)
            full_transcription += segment.text + " "
            words = []
            if hasattr(segment, 'words') and segment.words:
                for w in segment.words:
                    words.append({
                        "word": w.word,
                        "start": w.start,
                        "end": w.end
                    })

            results.append({
                "text": segment.text,
                "start": segment.start,
                "end": segment.end,
                "words": words
            })

        return jsonify({
            "originalAudio": audio_b64,
            "language": info.language,
            "transcription": full_transcription.strip(),
            "segments": results
        })

    except Exception as e:
        logger.error(f"Streaming error: {str(e)}")
        return jsonify({"error": str(e)}), 500


# Optional: Keep manual reset route if you still want it
@app.route('/reset_context', methods=['POST'])
def reset_context():
    transcription_context.clear()
    logger.info("Transcription context has been reset.")
    return jsonify({"message": "Transcription context reset."}), 200


# Translate endpoint
@app.route('/translate', methods=['POST', 'OPTIONS'])
def translate():
    if request.method == 'OPTIONS':
        return '', 200
    try:
        data = request.get_json()
        text = data.get('text')
        source_lang = data.get('sourceLanguage', 'auto')
        target_lang = data.get('targetLanguage')

        if not text or not target_lang:
            return jsonify({"error": "Missing required fields"}), 400

        source_code = LANGUAGES.get(source_lang, None) if source_lang.lower() not in ['auto', 'autodetect'] else None
        target_code = LANGUAGES[target_lang]

        result = translate_client.translate(
            text,
            target_language=target_code,
            source_language=source_code
        )

        translated_text = result['translatedText']
        voice_lang = TTS_VOICE_MAP.get(target_lang, 'en-US')

        synthesis_input = texttospeech.SynthesisInput(text=translated_text)
        voice = texttospeech.VoiceSelectionParams(
            language_code=voice_lang,
            ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
        )
        audio_config = texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.MP3
        )
        response = tts_client.synthesize_speech(
            input=synthesis_input,
            voice=voice,
            audio_config=audio_config
        )
        audio_b64 = base64.b64encode(response.audio_content).decode()

        return jsonify({
            "originalText": text,
            "sourceLanguage": source_lang,
            "targetLanguage": target_lang,
            "translatedText": translated_text,
            "audioContent": audio_b64
        })

    except Exception as e:
        logger.error(f"Translation error: {str(e)}")
        return jsonify({"error": str(e)}), 500

# Speech-to-Speech translation endpoint
@app.route('/speech-to-speech-translate', methods=['POST', 'OPTIONS'])
def speech_to_speech_translate():
    if request.method == 'OPTIONS':
        return '', 200
    try:
        # Check for the presence of audio file
        if 'audio' not in request.files:
            return jsonify({"error": "No audio file provided"}), 400

        audio_file = request.files['audio']
        source_lang = request.form.get('sourceLanguage', 'auto')
        target_lang = request.form.get('targetLanguage')

        # Validate target language
        if not target_lang or target_lang not in LANGUAGES:
            return jsonify({"error": "Invalid target language"}), 400

        # Read audio file into memory
        audio_bytes = audio_file.read()
        audio_file.seek(0)
        audio_b64 = base64.b64encode(audio_bytes).decode()  # Base64 encode for JSON response

        # Process the audio with BytesIO to pass into transcription
        audio_data = preprocess_audio(BytesIO(audio_bytes))

        # Transcribe the audio
        segments, info = model.transcribe(
            audio_data,
            language=None if source_lang.lower() in ['auto', 'autodetect'] else LANGUAGES[source_lang]
        )
        transcription = " ".join(segment.text for segment in segments)

        # Translate the transcription to the target language
        translate_args = {'target_language': LANGUAGES[target_lang]}
        if source_lang.lower() not in ['auto', 'autodetect']:
            translate_args['source_language'] = LANGUAGES[source_lang]

        translated_text = translate_client.translate(transcription, **translate_args)['translatedText']

        # Synthesize the translated text into speech
        synthesis_input = texttospeech.SynthesisInput(text=translated_text)
        voice = texttospeech.VoiceSelectionParams(
            language_code=TTS_VOICE_MAP[target_lang],
            ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
        )
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)
        response = tts_client.synthesize_speech(
            input=synthesis_input,
            voice=voice,
            audio_config=audio_config
        )
        synthesized_b64 = base64.b64encode(response.audio_content).decode()

        # Return JSON response with all necessary information
        return jsonify({
            "originalAudio": audio_b64,
            "transcription": transcription,
            "translatedText": translated_text,
            "synthesizedAudio": synthesized_b64,
            "sourceLanguage": info.language,
            "targetLanguage": target_lang
        })

    except Exception as e:
        logger.error(f"Speech-to-speech error: {str(e)}")
        return jsonify({"error": str(e)}), 500

# ========================
# SERVER STARTUP
# ========================

if __name__ == "__main__":
    # Warmup model
    logger.info("Warming up model...")
    model.transcribe(np.zeros((16000,), dtype=np.float32))

    # Start ngrok (with base64 encoded auth token)
    from pyngrok import ngrok, conf
    import base64

    # Decode ngrok auth token
    ngrok_auth = "2uiYZe14EThZNZIUshC4TBXyNPH_7dvs3mrwVtBUxQB1cXuCg"

    # Set auth token correctly
    conf.get_default().auth_token = ngrok_auth

    # Start tunnel
    public_url = ngrok.connect(5000)
    logger.info(f"Public URL: {public_url.public_url}")
    print(f"Public URL: {public_url.public_url}")

    # Fixed: Corrected indentation for app.run
    app.run(port=5000, threaded=True)